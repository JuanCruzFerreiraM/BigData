{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fc6fdb",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "Dejamos de pensar en jobs y fases, solamente entendemos una API, permite trabajar en RAM.\n",
    "\n",
    "Tenemos el spark core que se encarga de los mecanismos distribuidos, sobre este hay varios modulos, SQL, streaming, GraphX, Mllib.\n",
    "\n",
    "Es completamente portable, desde una maquina a un cluster.  \n",
    "\n",
    "## RDDs \n",
    "\n",
    "Bases distr. y resilientes, es una abstraccion, es tolerante a fallas. Las RDDs son inmutables, desde una podemos generar otra, se dividen en particiones.\n",
    "\n",
    "Creamos el spark content importando desde pyspark\n",
    "sc = SparkContext(\"local\", \"MyProgram\") si usáramos un cluster tenemos que modificar el local por la IP.\n",
    "\n",
    "Pensamos secuencial.\n",
    "\n",
    "Luego almacenamos los datos, clientes = sc.textFile(\"Clientes\") nos leer un archivo. Las variables guardan referencias a rdds, que para nosotros es un objeto. \n",
    "Spark trabaja con tuplas, en el txt devuelve un string hasta \\n.\n",
    "\n",
    "La idea es usar la RDDs original para pasar a otras, tenemos transformaciones y acciones\n",
    "\n",
    "### Transformaciones \n",
    "\n",
    "Funcion que hacer algo en una o varias RDDs y resulta en una o varias RDDs con relacion m->n. \n",
    "\n",
    "map para cada dato hace algo y produce otro dato n->n \n",
    "\n",
    "res = rdd.map(fmap) se pueden usar funciones lambda.\n",
    "\n",
    "filter n -> m con m <= n básicamente filtra.\n",
    "Utiliza al igual que map una function, rdd.filter(ffilter) se puede hacer con funciones lambda. \n",
    "\n",
    "\n",
    "distinct nos permite nos permite separar repetidas. \n",
    "\n",
    "Para ejecutar el DAG, usamos las acciones.\n",
    "\n",
    "### Acciones\n",
    "\n",
    "clients.count(), client.first() devuelve la primer tupla de la partición 0, take() toma una cantidad de tuplas aleatorias minimizando acceso a particiones.\n",
    "takeSample(True, value) toma una muestra de todos los nodos. top() los principales, collect() devuelve todos, esto es cuando no tenemos big data. Nos permite guardar en un saveAsTextFile o otros file.reduce() igual que la etapa reduce, resumir.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
