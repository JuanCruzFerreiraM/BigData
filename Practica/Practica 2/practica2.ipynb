{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc2c6aa",
   "metadata": {},
   "source": [
    "# Practica 2\n",
    "\n",
    "## Ejercicio 1\n",
    "\n",
    "sofmdosfm\n",
    "\n",
    "## Ejercicio 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67394e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import datetime,math,re\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from MRE import Job\n",
    "\n",
    "root_path = './' #Simplemente para poder usar rootpaths\n",
    "input_path = root_path + \"input/\"\n",
    "output_path = root_path + \"output/\"\n",
    "input_dir = input_path + \"eje2/\"\n",
    "output_dir = output_path + \"eje2/\"\n",
    "\n",
    "\n",
    "def fmap(key, value, context):\n",
    "    context.write(value, 1)\n",
    "    \n",
    "def fcomb(key, values, context):\n",
    "    c = 0\n",
    "    for v in values: \n",
    "        c += v\n",
    "    context.write(key, c)\n",
    "    \n",
    "def fred(key,values,context):\n",
    "    n = 0\n",
    "    for v in values:\n",
    "        n += v\n",
    "    context.write(key,n)\n",
    "\n",
    "job = Job(input_dir,output_dir,fmap,fred)\n",
    "job.setCombiner(fcomb)\n",
    "success = job.waitForCompletion()\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4a252",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Mi lógica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c45e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e350ed2b",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Utilice el dataset Libros para implementar una aplicación MapReduce que devuelva\n",
    "como salida todos los párrafos que tienen una longitud mayor al promedio. \n",
    "\n",
    "Considerando que la entrada es del tipo <key, párrafo>. Queremos saber cuantos cual es el párrafo con la mayor longitud.\n",
    "Lo que yo considero aca es que podemos implementar una solución de dos JOBS, en primer lugar el primer mapper simplemente hace algo estilo <1, [párrafo, len(párrafo)]> El reducer va a determinar el promedio, y su salida va a ser algo estilo (promedio, [párrafo,...]). Luego los mappers del siguiente Job simplemente evalúan si el párrafo es mayor que el promedio, y se lo mandan a un dummy reducer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8e0543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import datetime,math,re\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from MRE import Job\n",
    "\n",
    "root_path = './' #Simplemente para poder usar rootpaths\n",
    "input_path = root_path + \"input/\"\n",
    "output_path = root_path + \"output/\"\n",
    "input_dir = input_path + \"eje3/\"\n",
    "output_dir = output_path + \"eje4a/\"\n",
    "\n",
    "def fmap1 (key, values, context):\n",
    "    n = len(values)\n",
    "    context.write(1,[values, n])\n",
    "    \n",
    "def fred1 (key,values,context):\n",
    "    total_len = 0\n",
    "    n = 0\n",
    "    p_values = []\n",
    "    for value in values:\n",
    "        total_len += value[1]\n",
    "        n += 1\n",
    "        p_values.append(value[0])\n",
    "    for p in p_values:\n",
    "        context.write(total_len / n, p)\n",
    "\n",
    "job1 = Job(input_dir,output_dir,fmap1,fred1)\n",
    "success = job1.waitForCompletion()\n",
    "print(success)\n",
    "\n",
    "def fmap2 (key, value, context):\n",
    "    if (len(value) > float(key)):\n",
    "        context.write(value, 1)\n",
    "        \n",
    "def fred2 (key, values, context):\n",
    "    for v in values:\n",
    "        context.write(v, key)\n",
    "\n",
    "job2 = Job(output_dir,output_path + \"eje4b/\",fmap2,fred2)\n",
    "success = job2.waitForCompletion()\n",
    "print(success)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad160b",
   "metadata": {},
   "source": [
    "## Ejercicio 5 \n",
    "\n",
    "El dataset website tiene información sobre el tiempo de permanencia de sus usuarios\n",
    "en cada una de las páginas del sitio. El formato de los datos del dataset es:\n",
    "<id_user, id_page, time>\n",
    "Implemente una aplicación MapReduce, utilizando combiners en los casos que\n",
    "considere necesario, que calcule\n",
    "a. La página más visitada (la página en la que más tiempo permaneció) para cada\n",
    "usuario\n",
    "b. El usuario que más páginas distintas visitó\n",
    "c. La página más visitada (en cuanto a cantidad de visitas, sin importar el tiempo\n",
    "de permanencia) por todos los usuarios.\n",
    "Indique como queda el DAG del proceso completo (las tres consultas).\n",
    "\n",
    "Supongo que planteamos tres soluciones separadas \n",
    "\n",
    "### A\n",
    "\n",
    "Queremos saber cual es la pagina que mas visito cada usuario, para la primera entrada los mappers no pueden hacer mucho ya que no pueden manejar variables globales, ni tampoco saben si van a trabajar con toda la información de un usuario, lo que si podemos hacer es que el mapper a la salida informe <(id_us, id_page), time> de esta forma cada nodo reducer puede contar cuanto tiempo paso una usuario en una pagina, luego devuelve algo de la forma <(id_us, id_page), time_acum>, luego el mapper del siguiente Job lee la información la separa y la envía al reducer, uno por usuario, el cual simplemente busca el máximo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "882b6fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import datetime,math,re\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from MRE import Job\n",
    "\n",
    "root_path = './' #Simplemente para poder usar rootpaths\n",
    "input_path = root_path + \"input/\"\n",
    "output_path = root_path + \"output/\"\n",
    "input_dir = input_path + \"eje5\"\n",
    "output_dir1 = output_path + \"eje5a/Job1/\"\n",
    "output_dir2 = output_path + \"eje5a/Job2/\"\n",
    "\n",
    "def fmap1 (key, value, context):\n",
    "    user_id = key\n",
    "    data = value.split('\\t')\n",
    "    page_id = data[0]\n",
    "    page_time = data[1]\n",
    "    context.write((user_id,page_id), page_time)\n",
    "    \n",
    "def fred1 (key, value, context):\n",
    "    user_id, page_id = key\n",
    "    total_time = 0\n",
    "    for v in value:\n",
    "        total_time += float(v)\n",
    "    \n",
    "    context.write((user_id, page_id), total_time)\n",
    "    \n",
    "def fmap2 (key, value, context):\n",
    "    user_id = key\n",
    "    data = value.split('\\t')\n",
    "    page_id = data[0]\n",
    "    total_time = data[1]\n",
    "    context.write(user_id, (page_id, total_time))\n",
    "    \n",
    "def fred2 (key, value, context):\n",
    "    max_time = 0\n",
    "    max_id = 0\n",
    "    for v in value: \n",
    "        act_id = v[0]\n",
    "        act_time = float(v[1])\n",
    "        if (act_time > max_time):\n",
    "            max_time = act_time\n",
    "            max_id = act_id\n",
    "    context.write(key, (max_id, max_time))\n",
    "    \n",
    "    \n",
    "job1 = Job(input_dir, output_dir1, fmap1, fred1)\n",
    "print(job1.waitForCompletion())\n",
    "    \n",
    "job2 = Job(output_dir1, output_dir2, fmap2, fred2)\n",
    "success = job2.waitForCompletion()\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f6a53",
   "metadata": {},
   "source": [
    "### B\n",
    "\n",
    "Queremos saber el usuario que mas paginas visito, yo voy a considerar que no importa la cantidad de paginas totales y no la cantidad de paginas diferentes. Para esto planteo una solución de dos Jobs. \n",
    "\n",
    "1. El mapper se encarga de simplemente dar una salida estilo <user_id, 1> esto llega a un combiner que va a sumar las apariciones en estilo <user_id, count>, el reducer termina de totalizar dicho valor, <user_id, total>\n",
    "2. El mapper simplemente devuelve <1, (user_id, total)>, el reducer determina el máximo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "483bc5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime,math,re\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from MRE import Job\n",
    "\n",
    "root_path = './' #Simplemente para poder usar rootpaths\n",
    "input_path = root_path + \"input/\"\n",
    "output_path = root_path + \"output/\"\n",
    "input_dir = input_path + \"eje5\"\n",
    "output_dir1 = output_path + \"eje5b/Job1/\"\n",
    "output_dir2 = output_path + \"eje5b/Job2/\"\n",
    "\n",
    "def fmap1 (key, value, context):\n",
    "    user_id = key\n",
    "    context.write(user_id, 1)\n",
    "\n",
    "def fcom1 (key, value, context):\n",
    "    count = 0\n",
    "    for v in value:\n",
    "        count += v\n",
    "    context.write(key, count)\n",
    "\n",
    "def fred1(key,value,context):\n",
    "    total = 0\n",
    "    for v in value:\n",
    "        total += v\n",
    "    context.write(key, total)\n",
    "\n",
    "def fmap2 (key, value, context):\n",
    "    user_id = key\n",
    "    total = value\n",
    "    context.write(1, (user_id, total))\n",
    "\n",
    "def fred2 (key, value, context):\n",
    "    max_visits = 0\n",
    "    max_id = 0\n",
    "    \n",
    "    for v in value:\n",
    "        act_id = v[0]\n",
    "        act_visits = int(v[1])\n",
    "        if (act_visits > max_visits):\n",
    "            max_visits = act_visits\n",
    "            max_id = act_id\n",
    "    context.write(max_id, max_visits)\n",
    "\n",
    "job1 = Job(input_dir, output_dir1, fmap1, fred1)\n",
    "job1.setCombiner(fcom1)\n",
    "job1.waitForCompletion()\n",
    "\n",
    "job2 = Job(output_dir1, output_dir2, fmap2, fred2)\n",
    "job2.waitForCompletion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a618564f",
   "metadata": {},
   "source": [
    "### C\n",
    "\n",
    "Queremos saber ahora la pagina mas visitada, podemos hacer algo similar a la anterior dos jobs.\n",
    "\n",
    "1. El mapper devuelve <page_id, 1> luego un combiner suma estos valores y el reducer determina el total.\n",
    "2. El mapper ahora determina <1, (page_id, total)>, y el reducer calcula el máximo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57386148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime,math,re\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from MRE import Job\n",
    "\n",
    "root_path = './' #Simplemente para poder usar rootpaths\n",
    "input_path = root_path + \"input/\"\n",
    "output_path = root_path + \"output/\"\n",
    "input_dir = input_path + \"eje5\"\n",
    "output_dir1 = output_path + \"eje5c/Job1/\"\n",
    "output_dir2 = output_path + \"eje5c/Job2/\"\n",
    "\n",
    "def fmap1 (key, value, context):\n",
    "    page_id = value.split('\\t')[0]\n",
    "    context.write(page_id, 1)\n",
    "\n",
    "def fcom1 (key, value, context):\n",
    "    count = 0\n",
    "    for v in value:\n",
    "        count += v\n",
    "    context.write(key, count)\n",
    "\n",
    "def fred1(key,value,context):\n",
    "    total = 0\n",
    "    for v in value:\n",
    "        total += v\n",
    "    context.write(key, total)\n",
    "\n",
    "def fmap2 (key, value, context):\n",
    "    page_id = key\n",
    "    total = value\n",
    "    context.write(1, (page_id, total))\n",
    "\n",
    "def fred2 (key, value, context):\n",
    "    max_visits = 0\n",
    "    max_id = 0\n",
    "    \n",
    "    for v in value:\n",
    "        act_id = v[0]\n",
    "        act_visits = int(v[1])\n",
    "        if (act_visits > max_visits):\n",
    "            max_visits = act_visits\n",
    "            max_id = act_id\n",
    "    context.write(max_id, max_visits)\n",
    "\n",
    "job1 = Job(input_dir, output_dir1, fmap1, fred1)\n",
    "job1.setCombiner(fcom1)\n",
    "job1.waitForCompletion()\n",
    "\n",
    "job2 = Job(output_dir1, output_dir2, fmap2, fred2)\n",
    "job2.waitForCompletion()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
